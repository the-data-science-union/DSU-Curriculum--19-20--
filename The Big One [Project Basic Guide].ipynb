{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##EarthQuake Project.\n",
    "\n",
    "#The datasets given to you from the prompt are sufficient to forecast the next \"Big One\", that is how strong it will be and where it will take place and when it will be.\n",
    "#There are many ways to go about finding this, but for those struggling here is a basic guide to understand the process of how we can figure this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Nate'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Step 1 : Data Retrival\n",
    "\n",
    "The first thing your gonna want to do is import the Data, I see the the USGS Significant Earthquakes dataset has variables relating to magnitude,time, coordinates, and other things...\n",
    "\n",
    "So we are going to import this dataset using the Pandas Package (specifically pd.read_csv())\n",
    "\n",
    "Make sure you check where your file in working directory is: we will do this using the os (opering systems) package\n",
    "\n",
    "'''\n",
    "import os as os \n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Data = pd.read_csv(\"database.csv\") ##import the USGS Signficant Earthquake data recieved from Kaggle\n",
    "\n",
    "data = pd.DataFrame(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Type</th>\n",
       "      <th>Depth</th>\n",
       "      <th>Depth Error</th>\n",
       "      <th>Depth Seismic Stations</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>Magnitude Type</th>\n",
       "      <th>...</th>\n",
       "      <th>Magnitude Seismic Stations</th>\n",
       "      <th>Azimuthal Gap</th>\n",
       "      <th>Horizontal Distance</th>\n",
       "      <th>Horizontal Error</th>\n",
       "      <th>Root Mean Square</th>\n",
       "      <th>ID</th>\n",
       "      <th>Source</th>\n",
       "      <th>Location Source</th>\n",
       "      <th>Magnitude Source</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/02/1965</td>\n",
       "      <td>13:44:18</td>\n",
       "      <td>19.246</td>\n",
       "      <td>145.616</td>\n",
       "      <td>Earthquake</td>\n",
       "      <td>131.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>MW</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ISCGEM860706</td>\n",
       "      <td>ISCGEM</td>\n",
       "      <td>ISCGEM</td>\n",
       "      <td>ISCGEM</td>\n",
       "      <td>Automatic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/04/1965</td>\n",
       "      <td>11:29:49</td>\n",
       "      <td>1.863</td>\n",
       "      <td>127.352</td>\n",
       "      <td>Earthquake</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.8</td>\n",
       "      <td>MW</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ISCGEM860737</td>\n",
       "      <td>ISCGEM</td>\n",
       "      <td>ISCGEM</td>\n",
       "      <td>ISCGEM</td>\n",
       "      <td>Automatic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/05/1965</td>\n",
       "      <td>18:05:58</td>\n",
       "      <td>-20.579</td>\n",
       "      <td>-173.972</td>\n",
       "      <td>Earthquake</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.2</td>\n",
       "      <td>MW</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ISCGEM860762</td>\n",
       "      <td>ISCGEM</td>\n",
       "      <td>ISCGEM</td>\n",
       "      <td>ISCGEM</td>\n",
       "      <td>Automatic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/08/1965</td>\n",
       "      <td>18:49:43</td>\n",
       "      <td>-59.076</td>\n",
       "      <td>-23.557</td>\n",
       "      <td>Earthquake</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.8</td>\n",
       "      <td>MW</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ISCGEM860856</td>\n",
       "      <td>ISCGEM</td>\n",
       "      <td>ISCGEM</td>\n",
       "      <td>ISCGEM</td>\n",
       "      <td>Automatic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/09/1965</td>\n",
       "      <td>13:32:50</td>\n",
       "      <td>11.938</td>\n",
       "      <td>126.427</td>\n",
       "      <td>Earthquake</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.8</td>\n",
       "      <td>MW</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ISCGEM860890</td>\n",
       "      <td>ISCGEM</td>\n",
       "      <td>ISCGEM</td>\n",
       "      <td>ISCGEM</td>\n",
       "      <td>Automatic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      Time  Latitude  Longitude        Type  Depth  Depth Error  \\\n",
       "0  01/02/1965  13:44:18    19.246    145.616  Earthquake  131.6          NaN   \n",
       "1  01/04/1965  11:29:49     1.863    127.352  Earthquake   80.0          NaN   \n",
       "2  01/05/1965  18:05:58   -20.579   -173.972  Earthquake   20.0          NaN   \n",
       "3  01/08/1965  18:49:43   -59.076    -23.557  Earthquake   15.0          NaN   \n",
       "4  01/09/1965  13:32:50    11.938    126.427  Earthquake   15.0          NaN   \n",
       "\n",
       "   Depth Seismic Stations  Magnitude Magnitude Type    ...      \\\n",
       "0                     NaN        6.0             MW    ...       \n",
       "1                     NaN        5.8             MW    ...       \n",
       "2                     NaN        6.2             MW    ...       \n",
       "3                     NaN        5.8             MW    ...       \n",
       "4                     NaN        5.8             MW    ...       \n",
       "\n",
       "   Magnitude Seismic Stations  Azimuthal Gap  Horizontal Distance  \\\n",
       "0                         NaN            NaN                  NaN   \n",
       "1                         NaN            NaN                  NaN   \n",
       "2                         NaN            NaN                  NaN   \n",
       "3                         NaN            NaN                  NaN   \n",
       "4                         NaN            NaN                  NaN   \n",
       "\n",
       "   Horizontal Error  Root Mean Square            ID  Source Location Source  \\\n",
       "0               NaN               NaN  ISCGEM860706  ISCGEM          ISCGEM   \n",
       "1               NaN               NaN  ISCGEM860737  ISCGEM          ISCGEM   \n",
       "2               NaN               NaN  ISCGEM860762  ISCGEM          ISCGEM   \n",
       "3               NaN               NaN  ISCGEM860856  ISCGEM          ISCGEM   \n",
       "4               NaN               NaN  ISCGEM860890  ISCGEM          ISCGEM   \n",
       "\n",
       "  Magnitude Source     Status  \n",
       "0           ISCGEM  Automatic  \n",
       "1           ISCGEM  Automatic  \n",
       "2           ISCGEM  Automatic  \n",
       "3           ISCGEM  Automatic  \n",
       "4           ISCGEM  Automatic  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Step 2 : Data Preparation\n",
    "\n",
    "Now that we have our data we can now move in to the process of preparing the data for our analysis.\n",
    "\n",
    "It seems that our data is already pretty \"clean\" except for the \"Time\" variable so what we should focus on is extracting variables of interest\n",
    "\n",
    "We will check the column (variable) names of all the dataset to see then we will use exploration operations to take them out.\n",
    "'''\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We find the Date, Time, Longitude, Latitude, and Magnitude to be the primary variables of interest so we will extract these variables\n",
    "# We will do this using the loc method.\n",
    "\n",
    "data1 = data.loc[:,[\"Date\",\"Time\",\"Longitude\",\"Latitude\",\"Magnitude\"]]\n",
    "\n",
    "#we find each element of the \"Time\" vairbale to be a string, how can we change that for analysis?\n",
    "#First, we will take out all of the colons in the column using re.sub [check back to the Juiced Balls project or Stack Overflow for reference]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '1975-02-23T025841.000Z'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-37cedc525297>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Time\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\":\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Time\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdata1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Time\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Time\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#it seems that we can't do convert some srings because they are not fomatted the same way as the other observations (see observation 20650)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-37cedc525297>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Time\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\":\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Time\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdata1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Time\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Time\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#it seems that we can't do convert some srings because they are not fomatted the same way as the other observations (see observation 20650)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '1975-02-23T025841.000Z'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "data1[\"Time\"] = [re.sub(\":\",'',x) for x in data1[\"Time\"]]\n",
    "data1[\"Time\"] = [np.float64(x) for x in data1[\"Time\"]]\n",
    "\n",
    "#it seems that we can't do convert some srings because they are not fomatted the same way as the other observations (see observation 20650)\n",
    "# we will keep on dropping these elements until it works,you can see from date from the Value error code see let that be your reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '1975-02-23T025841.000Z'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-4cf5a4a7553a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdata1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Time\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mdata1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Time\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20650\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Time\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Time\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-4cf5a4a7553a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdata1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Time\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mdata1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Time\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20650\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Time\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Time\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '1975-02-23T025841.000Z'"
     ]
    }
   ],
   "source": [
    "data1[\"Time\"] =data1[\"Time\"].drop(index=20650)\n",
    "data1[\"Time\"] = [np.float64(x) for x in data1[\"Time\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '1985-04-28T025341.530Z'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-0804bd3e7f59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdata1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Time\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mdata1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Time\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3378\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Time\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Time\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-0804bd3e7f59>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdata1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Time\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mdata1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Time\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3378\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Time\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Time\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '1985-04-28T025341.530Z'"
     ]
    }
   ],
   "source": [
    "data1[\"Time\"] =data1[\"Time\"].drop(index=3378)\n",
    "data1[\"Time\"] = [np.float64(x) for x in data1[\"Time\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1[\"Time\"] =data1[\"Time\"].drop(index=7512)\n",
    "data1[\"Time\"] = [np.float64(x) for x in data1[\"Time\"]]\n",
    "\n",
    "#Woolah now its numeric!\n",
    "#This is not the ususal approach you take to working with time variables, but this approach is in your scope and somewhat accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nStep 3 : Data Exploration\\n\\nNow that we have the variables of interest, we can start exploring our data and find insights to help us predict the magnitude and location/time of the next big one.\\n\\nNow they're are many ways to do this, and I hope each one of you can figure out a more useful way than the guide.\\n\\nBut for the sake of those having trouble, one of the basic methods you can use is averages.\\n\\nWe will find the average of Time, Magnitude, Latitude, and Longitude.\\n\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Step 3 : Data Exploration\n",
    "\n",
    "Now that we have the variables of interest, we can start exploring our data and find insights to help us predict the magnitude and location/time of the next big one.\n",
    "\n",
    "Now they're are many ways to do this, and I hope each one of you can figure out a more useful way than the guide.\n",
    "\n",
    "But for the sake of those having trouble, one of the basic methods you can use is averages.\n",
    "\n",
    "We will find the average of Time, Magnitude, Latitude, and Longitude.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.6399607380701"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1[\"Longitude\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6790331197719317"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1[\"Latitude\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.882530753460003"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1[\"Magnitude\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117448.32115852876"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1[\"Time\"].mean()\n",
    "#117448 or in terms of time --> 120448 --> 12:04:48\n",
    "#We will leave the exploration analysis of Time and Date for you all, here's something to keep in mind..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Conclusion :\n",
    "    \n",
    " \"We find the earhtuqake of a magnitude of 5.88 (The next Big One) to occur at coordinates 39.63 Longitude and 1.679 Latitude at 12:04:48 pm\"\n",
    "\n",
    "You should talk about what makes your analysis sufficient but you should also talk about what makes it insufficient.\n",
    "\n",
    "In this case our mean was a good estimate of magnitude, latitude, and longitude. But time gets distorted since each minute resets at 60 instead of 100.\n",
    "\n",
    "There also a factor of date, can you predict the specific date? Also the ethics of location, is it okay if we assume the next earthquake will happen at these specific coordinates based off our statistical analysis?\n",
    "\n",
    "But this is good fundamental analysis! Understanding having an approach and some type of distinct analysis is already above the average. Its the creative approach that defines a good project, not necessarily its results.\n",
    "\n",
    "\n",
    "I hope you all understand this is a basic guide to get your brains working, please understand if you follow this approach directly, no prize for you!\n",
    "'''\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
